{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Question generation model compression using Onnx"
      ],
      "metadata": {
        "id": "AmPZ5xzhp-Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet onnx\n",
        "!pip install --quiet onnxruntime\n",
        "!pip install --quiet transformers==4.28.1\n",
        "!pip install --quiet fastt5"
      ],
      "metadata": {
        "id": "-NSh1Dxj6bqj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet sentencepiece\n",
        "!pip install --quiet gradio\n",
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi0CDI_CqCrP",
        "outputId": "0fd16208-13e1-440c-ada2-acb0ca877423"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 422 Âµs (started: 2024-10-11 19:09:02 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hICe3IAAqzhM",
        "outputId": "1f669acf-4cbb-43d3-ca2f-3d4ef0d76e4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "time: 24.4 s (started: 2024-10-11 17:41:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "import gradio as gr\n",
        "\n",
        "from fastT5 import export_and_get_onnx_model, generate_onnx_representation, quantize, get_onnx_model,get_onnx_runtime_sessions,OnnxT5\n",
        "from transformers import T5Config,AutoTokenizer\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "from onnxruntime.quantization import quantize_dynamic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvKaquNqrES2",
        "outputId": "4aa6d6f5-5968-4013-d4ea-e63cfe86c607"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 297 ms (started: 2024-10-11 19:09:02 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#T5 model size on disk ~ 900 MB\n",
        "question_model = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/Models/Q-tips/model')\n",
        "question_tokenizer = T5Tokenizer.from_pretrained('/content/drive/MyDrive/Models/Q-tips/model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06Y2izGLqIcr",
        "outputId": "166eaa80-8a54-41f3-d01b-fc4708c46ad0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 19.2 s (started: 2024-10-11 17:44:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_question(sentence,answer,mdl,tknizer):\n",
        "  text = \"context: {} answer: {}\".format(sentence,answer)\n",
        "  # print (text)\n",
        "  max_len = 256\n",
        "  encoding = tknizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = mdl.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=300)\n",
        "\n",
        "\n",
        "  dec = [tknizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n",
        "\n",
        "\n",
        "# context = \"Samrat loves to watch football during his free time\"\n",
        "# answer = \"football\"\n",
        "\n",
        "context = \"Donald Trump is an American media personality and businessman who served as the 45th president of the United States.\"\n",
        "answer = \"Donald Trump\"\n",
        "\n",
        "ques = get_question(context,answer,question_model,question_tokenizer)\n",
        "print (\"question: \",ques)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP4_rEjjrYa3",
        "outputId": "af024d6a-960e-4b80-cfda-f38f5bcad178"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question:  Who is the 45th president of the United States?\n",
            "time: 4.96 s (started: 2024-10-11 17:45:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an UI with Gradio"
      ],
      "metadata": {
        "id": "Teaa2DAhr2ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = gr.Textbox(lines=5, placeholder=\"Enter paragraph/context here...\")\n",
        "answer = gr.Textbox(lines=3, placeholder=\"Enter answer/keyword here...\")\n",
        "question = gr.Textbox(label=\"Question\")\n",
        "\n",
        "def generate_question(context, answer):\n",
        "    return get_question(context, answer, question_model, question_tokenizer)\n",
        "\n",
        "# Update interface to use gradio.Interface with updated components\n",
        "iface = gr.Interface(\n",
        "    fn=generate_question,\n",
        "    inputs=[context, answer],\n",
        "    outputs=question\n",
        ")\n",
        "\n",
        "iface.launch(debug=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "uLrVUq14r1fx",
        "outputId": "0c9df7df-da3d-472b-92b8-c6454217493e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5fcdc813bb59bdab32.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5fcdc813bb59bdab32.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.69 s (started: 2024-10-11 18:13:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert T5 Pytorch model to Onnx format and Quantize using FastT5 library"
      ],
      "metadata": {
        "id": "cX_W8XV0sSoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize(model_name):\n",
        "    # Convert PosixPath to string\n",
        "    model_name_str = str(model_name)  # Convert to string for slicing\n",
        "    output_model_name = f\"{model_name_str[:-5]}-quantized.onnx\"\n",
        "    quantize_dynamic(model_input=model_name_str, model_output=output_model_name, per_channel=True)\n",
        "    return output_model_name\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBSMPzOz3Ww7",
        "outputId": "0c44f04b-c3f0-459e-c36f-23d50f324af8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.03 ms (started: 2024-10-11 18:38:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_path = '/content/drive/MyDrive/Models/Q-tips/model'\n",
        "\n",
        "# Step 1. convert t5 model to onnx\n",
        "onnx_model_paths = generate_onnx_representation(trained_model_path)\n",
        "\n",
        "# Step 2. (recommended) quantize the converted model for fast inference and to reduce model size.\n",
        "quant_model_paths = [quantize(onnx_model_path) for onnx_model_path in onnx_model_paths]\n",
        "\n",
        "tokenizer_onnx = AutoTokenizer.from_pretrained(trained_model_path)\n",
        "config = T5Config.from_pretrained(trained_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFU0IY0gs2P_",
        "outputId": "42114a3e-48ba-4e10-f8f0-8ff41a7546b8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting to onnx... |################################| 3/3\n",
            "\u001b[?25hWARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4min 56s (started: 2024-10-11 18:38:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokenizer also into models folder\n",
        "tokenizer_onnx.save_pretrained('model/')\n",
        "config.save_pretrained('model/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrM9Jw08x1Po",
        "outputId": "004f58f0-6922-4276-aa84-91547dd4f7dc"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 32.6 ms (started: 2024-10-11 18:44:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove non-quantized onnx files**"
      ],
      "metadata": {
        "id": "Z1xXtowB5TCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f -r /content/models/*decoder.onnx\n",
        "!rm -f -r /content/models/*encoder.onnx\n",
        "!du -sh /content/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np2oRfMH5Mth",
        "outputId": "67cfdce1-8abb-4b4a-da1a-41bfa4a7e165"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "408M\t/content/models\n",
            "time: 518 ms (started: 2024-10-11 18:46:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/model/ '/content/drive/MyDrive/Models/Q-tips/onnx_model'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha8EjcfT5ZuD",
        "outputId": "81740a71-2406-4715-aae3-9aec24661bc0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.14 s (started: 2024-10-11 18:48:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Onnx Interface"
      ],
      "metadata": {
        "id": "19Nhunxd6Mvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.6.1 sentencepiece\n",
        "!pip install fastt5==0.0.4 --no-dependencies\n",
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T68s1eVtCmum",
        "outputId": "aad3e7d0-2680-4b95-b801-279934f614d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime==1.7.0 (from versions: 1.12.0, 1.12.1, 1.13.1, 1.14.0, 1.14.1, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.17.0, 1.17.1, 1.17.3, 1.18.0, 1.18.1, 1.19.0, 1.19.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime==1.7.0\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: fastt5==0.0.4 in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "time: 396 Âµs (started: 2024-10-11 19:29:36 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastT5 import get_onnx_model,get_onnx_runtime_sessions,OnnxT5\n",
        "from transformers import T5Tokenizer\n",
        "from pathlib import Path\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCeM7xo0DsPL",
        "outputId": "23742b6d-1ab4-45ba-a062-a5bf0dd0857b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 698 Âµs (started: 2024-10-11 19:32:40 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_path =  \"/content/drive/MyDrive/Models/Q-tips/onnx_model/model\"\n",
        "\n",
        "pretrained_model_name = Path(trained_model_path).stem\n",
        "\n",
        "\n",
        "encoder_path = os.path.join(trained_model_path,f\"{pretrained_model_name}-encoder-quantized.onnx\")\n",
        "decoder_path = os.path.join(trained_model_path,f\"{pretrained_model_name}-decoder-quantized.onnx\")\n",
        "init_decoder_path = os.path.join(trained_model_path,f\"{pretrained_model_name}-init-decoder-quantized.onnx\")\n",
        "\n",
        "model_paths = encoder_path, decoder_path, init_decoder_path\n",
        "model_sessions = get_onnx_runtime_sessions(model_paths)\n",
        "model = OnnxT5(trained_model_path, model_sessions)\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(trained_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0CCUDWx6Kf8",
        "outputId": "71c98429-b77f-4387-e929-975191069e35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.25 s (started: 2024-10-11 19:32:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_question(sentence,answer,mdl,tknizer):\n",
        "  text = \"context: {} answer: {}\".format(sentence,answer)\n",
        "  print (text)\n",
        "  max_len = 256\n",
        "  encoding = tknizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = mdl.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=300)\n",
        "\n",
        "\n",
        "  dec = [tknizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n",
        "\n",
        "\n",
        "# context = \"Ramsri loves to watch cricket during his free time\"\n",
        "# answer = \"cricket\"\n",
        "\n",
        "context = \"Donald Trump is an American media personality and businessman who served as the 45th president of the United States.\"\n",
        "answer = \"Donald Trump\"\n",
        "\n",
        "ques = get_question(context,answer,model,tokenizer)\n",
        "print (\"question: \",ques)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr-GmYOc9erH",
        "outputId": "787e9b4a-36f0-4d63-b4b0-dca713f3c68b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1 ms (started: 2024-10-11 19:34:15 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Iraq launched a barrage of 180 ballistic missiles aimed at Israel, marking a significant increase in military hostilities and raising fears of a broader conflict in the region.\"\n",
        "answer = \"ballistic\"\n",
        "\n",
        "ques = get_question(context, answer, model, tokenizer)\n",
        "print(\"question: \", ques)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "collapsed": true,
        "id": "xah4uzZr7EWy",
        "outputId": "10202250-454a-47f1-eb58-a48f404382aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context: Iraq launched a barrage of 180 ballistic missiles aimed at Israel, marking a significant increase in military hostilities and raising fears of a broader conflict in the region. answer: ballistic\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'OnnxT5' object has no attribute 'run'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a01db0be2618>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ballistic\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"question: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6b4c8edaf278>\u001b[0m in \u001b[0;36mget_question\u001b[0;34m(sentence, answer, mdl, tknizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Assuming mdl is a callable that runs inference with ONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Adjust this based on your ONNX model's input/output specification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     outputs = mdl.run(None, {\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'OnnxT5' object has no attribute 'run'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 135 ms (started: 2024-10-11 19:34:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "context = gr.Textbox(lines=5, placeholder=\"Enter paragraph/context here...\")\n",
        "answer = gr.Textbox(lines=3, placeholder=\"Enter answer/keyword here...\")\n",
        "question = gr.Textbox(label=\"Question\")\n",
        "\n",
        "def generate_question(context,answer):\n",
        "  return get_question(context,answer,model,tokenizer)\n",
        "\n",
        "# Update interface to use gradio.Interface with updated components\n",
        "iface = gr.Interface(\n",
        "    fn=generate_question,\n",
        "    inputs=[context, answer],\n",
        "    outputs=question\n",
        ")\n",
        "iface.launch(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "9x7dJWCS8J5C",
        "outputId": "af24fe2c-d02b-47e0-a002-4d5a5d0c29c2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3d76be2908b190f645.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3d76be2908b190f645.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.74 s (started: 2024-10-11 19:36:47 +00:00)\n"
          ]
        }
      ]
    }
  ]
}